{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 7.068872962487293e-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  pred  neighbor_min  neighbor_med  neighbor_max  \\\n",
      "0         1.000000e+00  3.448735e-01      0.999999      1.000000   \n",
      "1         2.303483e-01  1.373577e-02      0.356021      1.000000   \n",
      "2         1.214818e-02  5.842320e-03      0.022058      0.806485   \n",
      "3         7.544638e-03  5.842320e-03      0.010829      0.024479   \n",
      "4         9.081059e-03  8.041023e-03      0.011586      0.017962   \n",
      "...                ...           ...           ...           ...   \n",
      "23585333  1.923477e-03  2.356266e-07      0.000063      1.000000   \n",
      "23585334  8.860329e-07  2.356266e-07      0.000010      0.949198   \n",
      "23585335  1.000000e+00  9.134875e-06      0.974640      1.000000   \n",
      "23585336  1.729165e-04  1.150460e-06      0.001329      1.000000   \n",
      "23585337  1.000000e+00  1.915087e-03      0.999998      1.000000   \n",
      "\n",
      "          neighbor_mean  neighbor_sd      sqr_diff  output  \n",
      "0              0.887225     0.217816  6.646194e-16       0  \n",
      "1              0.502149     0.419060  5.923637e-01       1  \n",
      "2              0.077199     0.159575  1.475782e-04       1  \n",
      "3              0.011325     0.003963  3.969878e+00       1  \n",
      "4              0.012287     0.003123  3.963758e+00       1  \n",
      "...                 ...          ...           ...     ...  \n",
      "23585333       0.281961     0.447571  3.699765e-06       0  \n",
      "23585334       0.028021     0.162768  7.850543e-13       0  \n",
      "23585335       0.653496     0.471560  5.029795e-25       0  \n",
      "23585336       0.373311     0.476768  2.990011e-08       0  \n",
      "23585337       0.761175     0.411599  7.851505e-19       0  \n",
      "\n",
      "[23585338 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"/Users/HaoWoo/Desktop/Bio_Research/training_data.csv\")\n",
    "data[\"output\"] = 0\n",
    "data.loc[data[\"sqr_diff\"]>=cutoff,\"output\"] = 1\n",
    "print (data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 20047537)\n",
      "(1, 20047537)\n",
      "(6, 3537801)\n",
      "(1, 3537801)\n"
     ]
    }
   ],
   "source": [
    "data_samples = data.shape[0]\n",
    "\n",
    "X_train = (data.iloc[0:int(data_samples*85/100),0:6]).to_numpy().T\n",
    "Y_train = (data.iloc[0:int(data_samples*85/100),7]).to_numpy().reshape([int(data_samples*85/100),1]).T\n",
    "\n",
    "X_test = (data.iloc[int(data_samples*85/100):,0:6]).to_numpy().T\n",
    "Y_test = (data.iloc[int(data_samples*85/100):,7]).to_numpy().reshape([data_samples-int(data_samples*85/100),1]).T\n",
    "\n",
    "print (X_train.shape) #(6, 20047537)\n",
    "print (Y_train.shape) #(1, 20047537)\n",
    "print (X_test.shape) #(6, 3537801)\n",
    "print (Y_test.shape) #(6, 3537801)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "Activating Function - Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    s = 1/(1+np.exp(-z))\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "\n",
    "    w = np.zeros((dim,1))\n",
    "    b = 0\n",
    "    \n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (6, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (6, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    A = sigmoid(np.dot(w.T,X)+b)                                    # compute activation\n",
    "    cost = (np.dot(Y,(np.log(A)).T)+np.dot(1-Y, (np.log(1-A)).T))/m                                # compute cost\n",
    "\n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    dw = np.dot(X, (A-Y).T)/m\n",
    "    db = np.sum(A-Y)/m\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (6, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (6, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in tqdm(range(num_iterations)):\n",
    "        \n",
    "        \n",
    "        # Cost and gradient calculation\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule\n",
    "        w = w - learning_rate*dw\n",
    "        b = b - learning_rate*db\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (6, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (6, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    A = sigmoid(np.dot(w.T,X)+b)\n",
    "    \n",
    "    for i in tqdm(range(A.shape[1])):\n",
    "        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        if A[0][i] <= 0.5:\n",
    "            Y_prediction[0][i] = 0\n",
    "        else:\n",
    "            Y_prediction[0][i] = 1\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 500, learning_rate = 0.5, print_cost = False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function implemented above\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (6, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (6, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize parameters with zeros\n",
    "    w, b = initialize_with_zeros(np.shape(X_train)[0])\n",
    "\n",
    "    # Gradient descent (≈ 1 line of code)\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = True)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "    \n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd8c332f3a246e7bd2673659ee882a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: -0.693147\n",
      "Cost after iteration 100: -0.535369\n",
      "Cost after iteration 200: -0.530648\n",
      "Cost after iteration 300: -0.528553\n",
      "Cost after iteration 400: -0.527239\n",
      "Cost after iteration 500: -0.526256\n",
      "Cost after iteration 600: -0.525440\n",
      "Cost after iteration 700: -0.524725\n",
      "Cost after iteration 800: -0.524083\n",
      "Cost after iteration 900: -0.523499\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90c1fef5a9349b3af42e166b968bea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3537801), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ee596ed20e4e08aebc4ad4c9c33862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20047537), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train accuracy: 75.07921297264596 %\n",
      "test accuracy: 75.7514908272116 %\n"
     ]
    }
   ],
   "source": [
    "d = model(X_train, Y_train, X_test, Y_test, num_iterations = 1000, learning_rate = 0.5, print_cost = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAEWCAYAAABfdFHAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XuYHNV95vHvO1dpdEH3CxchAwIJXwLegZglJCzgJCbeQLzgxZvY2GuW4I13s3GcGK83ttcb76MEJ068xIkxGIsNcXAwAdbBsYVsxyQ2tsXNkGljCXFT1LoiaWY00lx6fvtHnZZaQ480kqanpnvez/P001WnTlWfalC9U1WnTykiMDMzq5WmvBtgZmaNzUFjZmY15aAxM7OactCYmVlNOWjMzKymHDRmZlZTDhqzcSLpa5Kuz7sdZpONg8bqnqQXJF2Rdzsi4i0RsSbvdgBI+rakGybgc9olfUFSt6Stkj5whLrvllSS1FvxurTWbbT8teTdALN6IKklIobybgdMrrYAHwdWAKcDS4BvSeqKiL8fpf73IuJnJqpxNjn4jMYamqS3SnpS0h5J35X0hoplN0t6TlKPpC5Jv1Kx7N2S/knSpyW9Anw8lf2jpE9J2i3peUlvqVjn4FnEGOq+RtJ30mc/LOnPJP3lKPtwqaTNkj4kaStwp6S5kr4qaUfa/lclnZrqfxK4BLg1nTXcmspXSlor6RVJz0p6+zh8xe8C/ldE7I6IAvB54N3jsF1rIA4aa1iS3gh8Afh1YD7wOeBBSe2pynNkB+STgP8J/KWkpRWb+GlgE7AI+GRF2bPAAuAPgTskaZQmHKnuXwE/SO36OPDOo+zOEmAe2ZnDjWT/du9M88uA/cCtABHxEeAR4P0RMTMi3i9pBrA2fe4i4B3AZyW9ttqHSfpsCudqrx+lOnOBk4GnKlZ9Cqi6zeR8STsl/UTS70nyVZUpwEFjjew/AZ+LiO9HRCndP+kH3gQQEX8TEVsiYjgi7gE2ABdWrL8lIv5PRAxFxP5U9mJEfD4iSsAaYCmweJTPr1pX0jLgAuCjETEQEf8IPHiUfRkGPhYR/RGxPyJ2RcRXIqIvInrIgvDnjrD+W4EXIuLOtD+PA18BrqlWOSL+c0TMGeVVPiucmd73Vqy6F5g1Shu+A7yOLOj+HVnY/c5R9tsagIPGGtnpwG9X/jUOnEb2VziS3lVxWW0P2UFwQcX6L1fZ5tbyRET0pcmZVeodqe7JwCsVZaN9VqUdEXGgPCOpQ9LnJL0oqZvsID5HUvMo658O/PSI7+JXyc6Ujldvep9dUTYb6KlWOSI2RcTzKdifBj7BKEFnjcVBY43sZeCTI/4a74iIL0k6nex+wvuB+RExB3gGqLwMVquhzYvAPEkdFWWnHWWdkW35beAc4KcjYjbws6lco9R/GfiHEd/FzIh4X7UPk/QXI3qHVb7+GSAidqd9+amKVX8K+Oej7EvlPo122dEaiIPGGkWrpGkVrxayILlJ0k8rM0PSL0maBcwgO9DtAJD0HrIzmpqLiBeB9WQdDNokXQT822PczCyy+zJ7JM0DPjZi+TbgjIr5rwJnS3qnpNb0ukDSqlHaeFMKomqvynswdwH/I3VOWEl2ufKL1bYp6S2SFqfplcDvAQ8c435bHXLQWKN4iOzAW359PCLWkx34bgV2AxtJPaIiogv4I+B7ZAfl1wP/NIHt/VXgImAX8PvAPWT3j8bqT4DpwE7gUWBkd+I/Ba5JPdI+k+7j/DxwHbCF7LLeHwDtnJiPkXWqeBH4B+CWctdmScvSGdCyVPdy4EeS9pH997oP+N8n+PlWB+QHn5nlT9I9wI8jYuSZiVnd8xmNWQ7SZaszJTVJ+kXgKuD+vNtlVgvuw26WjyVkl47mA5uB90XEE/k2yaw2fOnMzMxqypfOzMyspnzpDFiwYEEsX74872aYmdWVxx57bGdELDxaPQcNsHz5ctavX593M8zM6oqkF8dSz5fOzMysphw0ZmZWUw4aMzOrKQeNmZnVlIPGzMxqykFjZmY15aAxM7OayuV3NOn5GfcAy4EXgLenhyiNrFcCnk6zL0XEL6fyu4FOYJDsueu/HhGDki4le77F82md+yLiE7XbEzOz2ogIhoaDoVIwODzM4NAwQ8PBYGmYoVIwNDzMYClbPlAaZqhUfXl5fnA4vZfrDQ0zOBxcsHwul6w46m8uT0heP9i8GVgXEasl3ZzmP1Sl3v6IOK9K+d3Ar6XpvwJuAP48zT8SEW8d7wabWX2KCErDwWApGBgazg7Kw8MMDsWrp0uHDs6DVaaHhocZGCof4LPygVGmR9vO0HCkbQwfCpLDDv7loJiYcSjfd+mZDRs0VwGXpuk1wLepHjRVRcRD5WlJPwBOHce2mdkxigj600F8YOjQq788XSrRnw7Qg6neYLluKftrfTD9ZV4+CB9aHgeny+X9Q4cfwAdGbLPycwZKw9Ry7OC25iZam0VLcxOtzU20HZwWramsPN3R1nKwbltzEy3NoqWpvP6h+i1NaRtNorUlm29N9VubmirqZusftu5hddL2W7JttYxY3tIkpNo/TTuvoFkcEUWAiChKWjRKvWmS1gNDwOqIOOx5HZJagXcCv1lRfJGkp8ieIvjBiKj6/HJJNwI3AixbtqxaFbNJLyI7OPcPDXNgsET/4DD9QyUODKb5VF4+EPcPDtNfqgyC0qFgKB0eDoeCo3QoMCq2c1iolIbHdb8qD9JtLU0HD+bl+dZ0oJ7e2szsaS1Z3ZYm2ssH9hbR1tyc3tP6LYeCoPKgnB3ws+1XTo8MiYPTLU20pnBonqADdb2rWdBIepjsmRsjfeQYNrMsIrZIOgP4pqSnI+K5iuWfBb4TEY+k+ceB0yOiV9KVZA+SWlFtwxFxG3AbQGdnp5+VYOOiNBz0DQyxf6DEvoESBwZLhx3wD6Qg6B8c5kD5fbB0aHqoXGf4sHX7K9Y9ULnuUOmE/1pvaVJ2ME8H9LaWJtpbmmhrac6mm5uY2d5CW0fTYfXaW5toa24+WNZ+cL3K7TS/atttB4Og6fDgaEkH8qYmmpp88G4kNQuaiLhitGWStklams5mlgLbR9nGlvS+SdK3gfPJnk+OpI8BC4Ffr6jfXTH9kKTPSloQETvHY5+scQyVhukbLGWB0D9E30CJvoES+8oh0T/E/sES+/pL9A2Ul2fv1cr60jr9Q8f3l32TYFprc/ZqaaK9tZn2liampfc5HW1Ma80O3NNaD5WX12lP60yreK8sb2+pDJAm2isCotkHdauxvC6dPQhcD6xO7w+MrCBpLtAXEf2SFgAXA3+Ylt0A/AJweUQMV6yzBNgWESHpQrLu27tqvTM2sQaGhtm7f5A9fQPs2T/I7n3Z+56+Afb0DbJn/yC9B4boGxjKQmGwRF//oWDYN5BdLhorCTpam+lob6GjrZmOtux91rQWlsyelpW1Hyqf0dbC9LZmOtqamT4yCEYJi9Zm/9LAGldeQbMa+LKk9wIvAdcCSOoEboqIG4BVwOckDZMFxuqI6Err/wXwIvC9dH203I35GuB9koaA/cB14UeITlpDpRQYFSGxu68yMAbY3TfI3vL0vkH27h+kt39o1G02N4k501uZNa3l4IH/pOmtLJ09jY72LAQqw6JcNr0iIGaMKJvW2uTr8GYnwI9yJrtH4+fRnJiIYHtPP1v27H9VcOztywLj8PIBeg6MHhhNgpOmtzKno405Ha3Mmd7K3I42TurI3ud0pGXTK+dbmdne4lAwmyCSHouIzqPV84PP7JgMDwf/smc/G7f3snF7Lxu296T33lGDY/a0FubOOBQKr1kwIwuN6a3M7agIk462bH56G7OmtfiGsFmDcNBYVUOlYV56pY8NKVDKofLc9n3sHywdrLdgZhtnLpzJVeedzIpFszh17vRDgZHCxDebzaY2B80U1z9U4oWdfWzY3sOGbb1s3NHLxm29PL9z32G/jVh60jTOWjST6y6cx4pFszhr0UzOWjSTeTPacmy9mdUDB80U0TcwxKYd+w4FSjpLefGVPkppqAsJTpvbwYpFM7n0nIWctWgmKxbP4syFM5g1rTXnPTCzeuWgaTB79w+ycXsvz6VLXeVLX5t37z9Yp6VJnD6/g7MXz+KX3rD04NnJmQtnMq21OcfWm1kjctA0gCdf3sOnvv4sG7b3sK27/2B5W0sTZyyYwfnL5vL2ztOyM5RFMzl9/gzaWvy7DTObGA6aBvDXP3iJ9S++wpWvX8qKRbNYkc5QTpvX4RvxZpY7B00D6Cp288Zlc/njt1d7ooKZWb58/aTODZWGeXZrD6uWzs67KWZmVTlo6twLu/bRPzTsoDGzSctBU+e6ij0AnOugMbNJykFT5wrFblqbxVmLZubdFDOzqhw0da5Q7ObMhTPdXdnMJi0fnepcodjty2ZmNqk5aOrYK/sG2Nbd744AZjapOWjqWKGYPbnaQWNmk5mDpo4dCppZObfEzGx0Dpo61lXsZtGsdubPbM+7KWZmo3LQ1LGuLd2+bGZmk56Dpk4NDA3z3I5eB42ZTXq5BI2keZLWStqQ3ueOUq8k6cn0erCi/IuSnq9Ydl4ql6TPSNoo6UeS3jhR+zTRNm7vZbAUnHuyg8bMJre8zmhuBtZFxApgXZqvZn9EnJdevzxi2e9ULHsylb0FWJFeNwJ/XovGTwbljgDnuiOAmU1yeQXNVcCaNL0GuHoct3tXZB4F5khaOk7bnlQKxW7aW5pYPn9G3k0xMzuivIJmcUQUAdL7olHqTZO0XtKjkkaG0SfT5bFPSyp3uzoFeLmizuZU9iqSbkzbXr9jx44T2JV8FLZ2c86SWbQ0+zabmU1uNTtKSXpY0jNVXlcdw2aWRUQn8B+AP5F0Zir/MLASuACYB3yo/LFVthHVNhwRt0VEZ0R0Lly48BialL+IoFDsYdUS358xs8mvZk/YjIgrRlsmaZukpRFRTJe2to+yjS3pfZOkbwPnA8+Vz4aAfkl3Ah9M85uB0yo2cSqw5cT2ZPLZ3tPPK/sG/ENNM6sLeV13eRC4Pk1fDzwwsoKkueVLYpIWABcDXWl+aXoX2f2dZyq2+67U++xNwN6KUGoYXR56xszqSM3OaI5iNfBlSe8FXgKuBZDUCdwUETcAq4DPSRomC8TVEdGV1r9b0kKyS2VPAjel8oeAK4GNQB/wngnanwlV7nG20kFjZnUgl6CJiF3A5VXK1wM3pOnvAq8fZf3LRikP4DfGr6WTU9eWbk6ZM52Tprfm3RQzs6Nyl6U6VCh66Bkzqx8OmjpzYLDE8zv3+YeaZlY3HDR15tmtPQwHHnrGzOqGg6bO+GFnZlZvHDR1plDsZkZbM6fN7ci7KWZmY+KgqTOFYg8rl86mqanaIAhmZpOPg6aORASFrd0eEcDM6oqDpo5s3r2fngNDvj9jZnXFQVNH3BHAzOqRg6aOdBW7kWDlEl86M7P64aCpI4ViN8vnz6CjLa8h6szMjp2Dpo4Uij3uCGBmdcdBUyd6Dgzy0it9ftiZmdUdB02deHZrD+ChZ8ys/jho6oR7nJlZvXLQ1ImuYg8nTW9l6UnT8m6KmdkxcdDUiewZNLPInl5tZlY/HDR1oDQcPLu1x5fNzKwuOWjqwIu79rF/sOSgMbO65KCpA4Vi6nHmoDGzOpRL0EiaJ2mtpA3pfe4o9UqSnkyvByvKH6ko3yLp/lR+qaS9Fcs+OlH7VEtdxb00N4mzFs3MuylmZscsr7FMbgbWRcRqSTen+Q9Vqbc/Is4bWRgRl5SnJX0FeKBi8SMR8dbxbnCeCsUezlw4g2mtzXk3xczsmOV16ewqYE2aXgNcfTwbkTQLuAy4f5zaNSllPc582czM6lNeQbM4IooA6X3RKPWmSVov6VFJ1cLoV8jOjLoryi6S9JSkr0l67WgNkHRj2vb6HTt2HPeO1NqevgGKew/4/oyZ1a2aXTqT9DCwpMqijxzDZpZFxBZJZwDflPR0RDxXsfwdwO0V848Dp0dEr6Qryc50VlTbcETcBtwG0NnZGcfQpgnV5REBzKzO1SxoIuKK0ZZJ2iZpaUQUJS0Fto+yjS3pfZOkbwPnA8+lbcwHLiQ7qynX766YfkjSZyUtiIid47FPeSj3OHPQmFm9yuvS2YPA9Wn6eg6/mQ+ApLmS2tP0AuBioKuiyrXAVyPiQMU6S5R+Oi/pQrL921WTPZgghWI3C2a2s3BWe95NMTM7LnkFzWrgzZI2AG9O80jqlFS+FLYKWC/pKeBbwOqIqAya64AvjdjuNcAzaZ3PANdFxKS9LDYW5aFnzMzqVS7dmyNiF3B5lfL1wA1p+rvA64+wjUurlN0K3DpuDc3ZYGmYDdt6ec/Fy/NuipnZcfPIAJPYph37GCgN+/6MmdU1B80k1lXcC7gjgJnVNwfNJFYo9tDW3MQZC2fk3RQzs+PmoJnECsVuViyeSWuz/zOZWf3yEWwS89AzZtYIHDST1PaeA+zsHfDQM2ZW9xw0k5RHBDCzRuGgmaQKaYwzn9GYWb1z0ExShWI3J580jZM6WvNuipnZCXHQTFLuCGBmjcJBMwkdGCzx3I59DhozawgOmklo4/ZeSsPhoDGzhuCgmYS6tpQfduZRm82s/jloJqGuYjfTW5s5fb6HnjGz+uegmYQKxW7OWTKL5ibl3RQzsxPmoJlkIsI9zsysoThoJpktew/QfWCIc0920JhZY3DQTDKFLeURAdwRwMwag4NmkikPPXPOEp/RmFljcNBMMoWt3Zw+v4OZ7S15N8XMbFzkFjSS5klaK2lDep87Sr1lkr4hqSCpS9LyVP4aSd9P698jqS2Vt6f5jWn58onap/FQKPawymczZtZA8jyjuRlYFxErgHVpvpq7gFsiYhVwIbA9lf8B8Om0/m7gvan8vcDuiDgL+HSqVxf29Q/xwi4PPWNmjSXPoLkKWJOm1wBXj6wg6VygJSLWAkREb0T0SRJwGXBvlfUrt3svcHmqP+n9eGsPER4RwMwaS55BszgiigDpfVGVOmcDeyTdJ+kJSbdIagbmA3siYijV2wyckqZPAV5O2x0C9qb6h5F0o6T1ktbv2LFjXHfseJU7AviMxswaSU3vOEt6GFhSZdFHxriJFuAS4HzgJeAe4N3Ag1XqRvljj7DsUEHEbcBtAJ2dna9anodCsZtZ01o4de70vJtiZjZuxnRGI+nasZSNFBFXRMTrqrweALZJWpq2tZRD914qbQaeiIhN6ezkfuCNwE5gjqRyUJ4KbKlY57S03RbgJOCVsexn3grFblYtmU2dXOkzMxuTsV46+/AYy47Fg8D1afp64IEqdX4IzJW0MM1fBnRFRADfAq6psn7ldq8BvpnqT2rDw8GPt/Z4RAAzazhHvHQm6S3AlcApkj5TsWg2MFR9rTFbDXxZ0nvJLotdmz6zE7gpIm6IiJKkDwLr0g39x4DPp/U/BPy1pN8HngDuSOV3AP9X0kayM5nrTrCdE+KlV/roGyi5I4CZNZyj3aPZAqwHfpnsIF/WA/zWiXxwROwCLq9Svh64oWJ+LfCGKvU2kXV3Hll+gBRa9cQdAcysUR0xaCLiKeApSX8VEYMA6YeVp0XE7olo4FRRKHbTJDh7sc9ozKyxjPUezVpJsyXNA54C7pT0xzVs15TTVezhjIUzmdbanHdTzMzG1ViD5qSI6AbeBtwZEf8KuKJ2zZp6/AwaM2tUYw2altQF+e3AV2vYnilp7/5B/mXPfncEMLOGNNag+QTwdeC5iPihpDOADbVr1tTijgBm1sjGNDJARPwN8DcV85uAf1erRk015aA510FjZg1orCMDnCrpbyVtl7RN0lcknVrrxk0VhWI382a0sWhWe95NMTMbd2O9dHYn2S/uTyYbtPL/pTIbB4ViD6uWzvLQM2bWkMYaNAsj4s6IGEqvLwILj7aSHd1QaZhnt/X4spmZNayxBs1OSb8mqTm9fg3YVcuGTRXP79zHwNCwOwKYWcMaa9D8R7KuzVuBItlgle+pVaOmki73ODOzBjfW59H8L+D68rAzaYSAT5EFkJ2AQrGH1mZx5sKZeTfFzKwmxnpG84bKsc0i4hWyh5HZCSoUuzlr0SzaWvJ82KmZWe2M9ejWlAbTBA6e0dT06ZxTRTb0jEcEMLPGNdaw+CPgu5LuJXss8tuBT9asVVPEzt5+tvf0u8eZmTW0sY4McJek9WRPuBTwtojoqmnLpgAPPWNmU8GYL3+lYHG4jCMHjZlNBb4DnaNCsYfFs9uZN6Mt76aYmdWMgyZHfgaNmU0FDpqc9A+V2Li91x0BzKzh5RI0kuZJWitpQ3qfO0q9ZZK+IakgqUvS8lR+t6RnJT0j6QuSWlP5pZL2SnoyvT46cXt1bDZu72VoOHxGY2YNL68zmpuBdRGxAliX5qu5C7glIlYBFwLbU/ndwErg9cB04IaKdR6JiPPS6xM1af04KBR7AHcEMLPGl1fQXAWsSdNrgKtHVpB0LtASEWsBIqI3IvrS9EORAD8A6u7ZOIViN9Nam3jNghl5N8XMrKbyCprFEVEESO+LqtQ5G9gj6T5JT0i6RVJzZYV0yeydwN9XFF8k6SlJX5P02lrtwIkqFLs5Z/Esmpv8DBoza2w1G0ZG0sPAkiqLPjLGTbQAl5CNqfYScA/wbuCOijqfBb4TEY+k+ceB0yOiV9KVwP3AilHadyNwI8CyZcvG2KTxEREUit38wmurfT1mZo2lZmc0EXFFRLyuyusBYJukpQDpfXuVTWwGnoiITRExRBYabywvlPQxsoevfaDiM7sjojdNPwS0SlowSvtui4jOiOhcuHBin+G2tfsAu/sGfX/GzKaEvC6dPQhcn6avBx6oUueHwFxJ5RS4jDQygaQbgF8A3hERw+UVJC1Reh6ypAvJ9m/SPaDNIwKY2VSSV9CsBt4saQPw5jSPpE5JtwNERAn4ILBO0tNkY6x9Pq3/F8Bi4HsjujFfAzwj6SngM8B1qcPApFLucbbSozab2RSQy1D/EbELuLxK+XoquiqnHmdvqFKvarsj4lbg1vFraW10Fbs5de50Zk9rzbspZmY155EBclAodntEADObMhw0E2z/QIkXdu7z/RkzmzIcNBPs2W09DIc7ApjZ1OGgmWDlHme+dGZmU4WDZoIVit3MbG/h1LnT826KmdmEcNBMsEKxm5VLZtHkoWfMbIpw0Eyg4eGgUOzx/Rkzm1IcNBNo8+799PYPOWjMbEpx0EygroNDz3hEADObOhw0E6hQ7EaCc5Y4aMxs6nDQTKBCsZvXzJ9BR1suI/+YmeXCQTOBClu7WXWy78+Y2dTioJkgPQcGefmV/f6hpplNOQ6aCfLjrdmjAdwRwMymGgfNBPHDzsxsqnLQTJBCsZs5Ha0smT0t76aYmU0oB80E6Sr2sGrJbNKTps3MpgwHzQQoDQfPbu32ZTMzm5IcNBPg+Z37ODA47I4AZjYlOWgmgDsCmNlUlkvQSJonaa2kDel97ij1lkn6hqSCpC5Jy1P5FyU9L+nJ9DovlUvSZyRtlPQjSW+cuL0aXaHYTUuTWLF4Zt5NMTObcHmd0dwMrIuIFcC6NF/NXcAtEbEKuBDYXrHsdyLivPR6MpW9BViRXjcCf16T1h+jQrGbMxfOpL2lOe+mmJlNuLyC5ipgTZpeA1w9soKkc4GWiFgLEBG9EdE3hu3eFZlHgTmSlo5ju49LodjDuR56xsymqLyCZnFEFAHS+6Iqdc4G9ki6T9ITkm6RVHlK8Ml0eezTktpT2SnAyxV1Nqey3OzeN8DW7gPuCGBmU1bNgkbSw5KeqfK6aoybaAEuAT4IXACcAbw7LfswsDKVzwM+VP7YKtuJUdp3o6T1ktbv2LFjjE06du4IYGZTXc3Gq4+IK0ZbJmmbpKURUUyXtrZXqbYZeCIiNqV17gfeBNxRPhsC+iXdSRZG5XVOq9jGqcCWUdp3G3AbQGdnZ9UwGg9dDhozm+LyunT2IHB9mr4eeKBKnR8CcyUtTPOXAV0A5fsuyn5mfzXwTMV235V6n70J2FsRSrkoFHtYOKudBTPbj17ZzKwB5fUErtXAlyW9F3gJuBZAUidwU0TcEBElSR8E1qVAeQz4fFr/7hRAAp4EbkrlDwFXAhuBPuA9E7VDo+kqekQAM5vacgmaiNgFXF6lfD1wQ8X8WuANVepdNsp2A/iN8WvpiRkYGmbj9h5+9uwFeTfFzCw3Hhmghp7b0ctgKfywMzOb0hw0NeQeZ2ZmDpqaKhS7aWtp4owFM/JuiplZbhw0NVQo9nD24pm0NPtrNrOpy0fAGokICsVu358xsynPQVMjO3r62bVvwPdnzGzKc9DUiEcEMDPLOGhqpFDsAWDVEgeNmU1tDpoaKRS7OWXOdE7qaM27KWZmuXLQ1Eg29IwfDWBm5qCpgQODJTbt6PX9GTMzHDQ18ZNtPQyHOwKYmYGDpiY89IyZ2SEOmhooFHvoaGvm9HkdeTfFzCx3Dpoa6Cp2s3LJLJqaqj1Z2sxsanHQjLPy0DO+bGZmlnHQjLN/2bOfngNDDhozs8RBM84OjgjgoDEzAxw0465Q7EaClUv8Y00zM3DQjLuuLd2cPq+DGe0teTfFzGxScNCMs8JWdwQwM6uUS9BImidpraQN6X3uKPWWSfqGpIKkLknLU/kjkp5Mry2S7k/ll0raW7HsoxO3V9DbP8SLu/ocNGZmFfI6o7kZWBcRK4B1ab6au4BbImIVcCGwHSAiLomI8yLiPOB7wH0V6zxSXhYRn6jdLrzas1s9IoCZ2Uh5Bc1VwJo0vQa4emQFSecCLRGxFiAieiOib0SdWcBlwP21be7YdB3sceaOAGZmZXkFzeKIKAKk90VV6pwN7JF0n6QnJN0iqXlEnV8hOzPqrii7SNJTkr4m6bWjNUDSjZLWS1q/Y8eOE90fIOtxNntaC6fMmT4u2zMzawQ16xol6WFgSZVFHxnjJlqAS4DzgZeAe4B3A3dU1HkHcHvF/OPA6RHRK+lKsjOdFdU2HhG3AbcBdHZ2xhjbdETlEQEkDz1jZlZWszOaiLgiIl5X5fUAsE3SUoD0vr3KJjYDT0TEpogYIguNN5YXSppPdt/m7yo+szsietP0Q0CrpAW12sdKw8PBs1t7fH/GzGyEvC6dPQhcn6bx90VvAAAKhElEQVSvBx6oUueHwFxJC9P8ZUBXxfJrga9GxIFygaQlSqcTki4k279d49z2ql58pY++gRLnOmjMzA6TV9CsBt4saQPw5jSPpE5JtwNERAn4ILBO0tOAgM9XbOM64EsjtnsN8Iykp4DPANdFxLhcFjsaP4PGzKy6XH6+HhG7gMurlK8HbqiYXwu8YZRtXFql7Fbg1nFr6DHo2tJNc5NYsXhmHh9vZjZpeWSAcVIodnPGghlMax3ZMc7MbGpz0IwTP4PGzKw6B8042NM3wJa9Bxw0ZmZVOGjGQcEjApiZjcpBMw7KPc7ctdnM7NUcNOOgUOxm/ow2Fs5qz7spZmaTjoNmHBS2dnPuyR56xsysGgfNCRoqDfOTbb3uCGBmNgoHzQnatHMfA0PD7ghgZjYKB80J8tAzZmZH5qA5QV1bumlrbuLMhR56xsysGgfNCeoqdnPWopm0NvurNDOrxkfHE1Qo+hk0ZmZH4qA5ATt6+tnZ2++OAGZmR+CgOQEeEcDM7OgcNCdgelszV6xa5EtnZmZHkMuDzxrFBcvnccHyeXk3w8xsUvMZjZmZ1ZSDxszMaspBY2ZmNZVb0EiaJ2mtpA3pfW6VOv9G0pMVrwOSrk7LXiPp+2n9eyS1pfL2NL8xLV8+sXtmZmaV8jyjuRlYFxErgHVp/jAR8a2IOC8izgMuA/qAb6TFfwB8Oq2/G3hvKn8vsDsizgI+neqZmVlO8gyaq4A1aXoNcPVR6l8DfC0i+pQ9+OUy4N4q61du917gcvlBMWZmuckzaBZHRBEgvS86Sv3rgC+l6fnAnogYSvObgVPS9CnAy2m7Q8DeVP8wkm6UtF7S+h07dpzQjpiZ2ehq+jsaSQ8DS6os+sgxbmcp8Hrg6+WiKtViDMsOFUTcBtwG0NnZ+arlZmY2PmoaNBFxxWjLJG2TtDQiiilIth9hU28H/jYiBtP8TmCOpJZ01nIqsCUt2wycBmyW1AKcBLxypHY+9thjOyW9OLa9epUFqT2W8fdxOH8fh/i7OFwjfB+nj6VSniMDPAhcD6xO7w8coe47gA+XZyIiJH2L7L7NX49Yv7zd76Xl34yII56xRMTC49wHJK2PiM7jXb/R+Ps4nL+PQ/xdHG4qfR953qNZDbxZ0gbgzWkeSZ2Sbi9XSt2TTwP+YcT6HwI+IGkj2T2YO1L5HcD8VP4BqvRmMzOziZPbGU1E7AIur1K+HrihYv4FDt3or6y3CbiwSvkB4NrxbKuZmR0/jwxw4m7LuwGTjL+Pw/n7OMTfxeGmzPeho9y+MDMzOyE+ozEzs5py0JiZWU05aE6ApF+U9GwawHNK926TdJqkb0kqSPpnSb+Zd5vyJqlZ0hOSvpp3W/ImaY6keyX9OP0/clHebcqLpN9K/0aekfQlSdPyblOtOWiOk6Rm4M+AtwDnAu+QdG6+rcrVEPDbEbEKeBPwG1P8+wD4TaCQdyMmiT8F/j4iVgI/xRT9XiSdAvxXoDMiXgc0kw2v1dAcNMfvQmBjRGyKiAGyH45elXObchMRxYh4PE33kB1IXtUtfaqQdCrwS8DtR6vb6CTNBn6W9Fu3iBiIiD35tipXLcD0NHJJB4dGNWlYDprjd3DwzqRyYM8pLf3I9nzg+/m2JFd/AvwuMJx3QyaBM4AdwJ3pUuLtkmbk3ag8RMS/AJ8CXgKKwN6I+MaR16p/DprjN6bBO6caSTOBrwD/LSK6825PHiS9FdgeEY/l3ZZJogV4I/DnEXE+sI8pOmJHesDjVcBrgJOBGZJ+Ld9W1Z6D5viVB+8sqxzYc0qS1EoWMndHxH15tydHFwO/LOkFskuql0n6y3yblKvNwOaIKJ/h3ksWPFPRFcDzEbEjDRJ8H/Cvc25TzTlojt8PgRXpkdJtZDf0Hsy5TblJD5e7AyhExB/n3Z48RcSHI+LUiFhO9v/FNyOi4f9qHU1EbAVelnROKroc6MqxSXl6CXiTpI70b+ZypkDHiDxHb65rETEk6f1kz8hpBr4QEf+cc7PydDHwTuBpSU+msv8eEQ/l2CabPP4LcHf6o2wT8J6c25OLiPi+pHuBx8l6aj7BFBiKxkPQmJlZTfnSmZmZ1ZSDxszMaspBY2ZmNeWgMTOzmnLQmJlZTTlorG5I+m56Xy7pP4zztv97tc+qFUlXS/pojbbdW6PtXnqiI1FL+qKka46w/P2SpmTX50bmoLG6ERHlX1AvB44paNJo20dyWNBUfFat/C7w2RPdyBj2q+bS4JDj5QtkoxtbA3HQWN2o+Et9NXCJpCfTsz2aJd0i6YeSfiTp11P9S9Mzcv4KeDqV3S/psfQ8kBtT2Wqy0XSflHR35Wcpc0t6dsjTkv59xba/XfGMlbvTL72RtFpSV2rLp6rsx9lAf0TsTPNflPQXkh6R9JM0Vlr5eTZj2q8qn/FJSU9JelTS4orPuaaiTm/F9kbbl19MZf8IvK1i3Y9Luk3SN4C7jtBWSbo1fR9/Byyq2MarvqeI6ANekHThWP6fsPrgkQGsHt0MfDAiygfkG8lGwb1AUjvwT+kACNnjHF4XEc+n+f8YEa9Img78UNJXIuJmSe+PiPOqfNbbgPPInqGyIK3znbTsfOC1ZGPc/RNwsaQu4FeAlRERkuZU2ebFZL8Mr7Qc+DngTOBbks4C3nUM+1VpBvBoRHxE0h8C/wn4/Sr1KlXbl/XA54HLgI3APSPW+VfAz0TE/iP8NzgfOAd4PbCYbOiZL0iad4TvaT1wCfCDo7TZ6oTPaKwR/DzwrjT0zfeB+cCKtOwHIw7G/1XSU8CjZIOiruDIfgb4UkSUImIb8A/ABRXb3hwRw8CTZGHRDRwAbpf0NqCvyjaXkg2bX+nLETEcERvIhmhZeYz7VWkAKN9LeSy162iq7ctKsgEgN0Q2hMjIgUEfjIj9aXq0tv4sh76/LcA3U/0jfU/byUY2tgbhMxprBAL+S0R8/bBC6VKyIekr568ALoqIPknfBo72GN1qj4Mo66+YLgEtaQy8C8kGS7wOeD/ZGUGl/cBJI8pGjgUVjHG/qhiMQ2NLlTj073yI9MdlujTWdqR9GaVdlSrbMFpbr6y2jaN8T9PIviNrED6jsXrUA8yqmP868D5ljylA0tmq/mCtk4DdKWRWkj1yumywvP4I3wH+fboHsZDsL/RRL+koex7PSWkw0f9GdtltpAJw1oiyayU1STqT7EFhzx7Dfo3VC2SXuyB7Jkq1/a30Y+A1qU0A7zhC3dHa+h3guvT9LQX+TVp+pO/pbOCZMe+VTXo+o7F69CNgKF0C+yLZ8+iXA4+nv9R3AFdXWe/vgZsk/YjsQP5oxbLbgB9JejwifrWi/G+Bi4CnyP4y/92I2JqCqppZwAOSppH9lf9bVep8B/gjSao483iW7LLcYuCmiDgg6fYx7tdYfT617QfAOo58VkRqw43A30naCfwj8LpRqo/W1r8lO1N5GvhJ2kc48vd0MfA/j3nvbNLy6M1mOZD0p8D/i4iHJX0R+GpE3Jtzs3In6XzgAxHxzrzbYuPHl87M8vG/gY68GzEJLQB+L+9G2PjyGY2ZmdWUz2jMzKymHDRmZlZTDhozM6spB42ZmdWUg8bMzGrq/wPITqmPV4XdHgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot learning curve (with costs)\n",
    "costs = np.squeeze(d['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate is: 0.25\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_set_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a7e780386766>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlearning_rates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"learning rate is: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"-------------------------------------------------------\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_set_x' is not defined"
     ]
    }
   ],
   "source": [
    "# tuning learning rate\n",
    "learning_rates = [0.25, 0.1, 0.05]\n",
    "models = {}\n",
    "for i in learning_rates:\n",
    "    print (\"learning rate is: \" + str(i))\n",
    "    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False)\n",
    "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
    "\n",
    "for i in learning_rates:\n",
    "    plt.plot(np.squeeze(models[str(i)][\"costs\"]), label= str(models[str(i)][\"learning_rate\"]))\n",
    "\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (hundreds)')\n",
    "\n",
    "legend = plt.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
